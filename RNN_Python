# Cargar las bibliotecas necesarias
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Embedding
from tensorflow.keras.optimizers import Adam

# Función para cargar y preprocesar los datos de entrenamiento
def load_and_preprocess_data():
    # Cargar los datos de texto
    with open("ruta/al/archivo/de/texto.txt", "r") as f:
        text = f.read()
    
    # Tokenizar el texto
    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=10000)
    tokenizer.fit_on_texts([text])
    sequences = tokenizer.texts_to_sequences([text])[0]
    
    # Obtener la longitud máxima de las secuencias
    max_length = max(len(seq) for seq in sequences)
    
    # Rellenar las secuencias a la longitud máxima
    X = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_length)
    
    # Convertir los índices de tokens a caracteres
    char_indices = tokenizer.word_index
    indices_char = list(char_indices.keys())
    
    return X, char_indices, indices_char

# Función para construir el modelo de RNN
def build_rnn_model(input_dim, output_dim, hidden_units=128, num_layers=2):
    model = Sequential()
    model.add(Embedding(input_dim=input_dim, output_dim=hidden_units))
    
    for _ in range(num_layers):
        model.add(LSTM(units=hidden_units, return_sequences=True))
    
    model.add(Dense(units=output_dim, activation="softmax"))
    
    return model

# Función para entrenar el modelo
def train_rnn_model(X, y, batch_size=128, epochs=50, validation_split=0.2):
    model = build_rnn_model(input_dim=X.shape[1], output_dim=y.shape[1])
    model.compile(optimizer=Adam(), loss="categorical_crossentropy", metrics=["accuracy"])
    
    history = model.fit(X, y, batch_size=batch_size, epochs=epochs, validation_split=validation_split)
    
    return model, history

# Función para generar texto usando el modelo entrenado
def generate_text(model, tokenizer, seed_text, max_length, diversity=1.0):
    char_indices = tokenizer.word_index
    indices_char = list(char_indices.keys())
    
    # Convertir el texto de inicio a una secuencia de índices
    input_seq = np.array([char_indices[char] for char in seed_text])
    input_seq = np.expand_dims(input_seq, axis=0)
    
    # Generar texto
    generated_text = seed_text
    for _ in range(max_length):
        preds = model.predict(input_seq)[0]
        next_index = np.random.choice(len(indices_char), p=np.exp(preds / diversity) / np.sum(np.exp(preds / diversity)))
        next_char = indices_char[next_index]
        generated_text += next_char
        
        # Actualizar la secuencia de entrada
        input_seq = np.roll(input_seq, -1, axis=1)
        input_seq[0, -1] = next_index
    
    return generated_text
